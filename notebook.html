<h1>Runbook for Building a Modern Data Warehouse</h1>

<h2>Code</h2>
https://github.com/ozkary/modern-data-warehouse-spark

<h2>
Oscar D. Garcia<h2></h2>
<h3>twitter: @ozkary</h3>
</h3>ozkary.com
</h3>

<h2>Command Line:</h2>
<ul>
     <li>
        Start a bash cli from the command line by typing:
        bash
    </li>
    <li> 
    Start the Thrift Server to enable client connections (hive metastore)  via JDBC or ODBC
    /opt/spark/sbin/start-thriftserver.sh 

    Note: Connect PowerBI with Spark via 
    HTTP http://localhost:10000/cliservice use ubunto username/pw
    </li>   
    <li><h2>Scala instructions</h2>
    <ul>       
        <li>
            Note: the location of your project as this is where scala/python files are located 
            i.e. spark/scala
        </li>
        <li>
        Start spark shell with the sql driver path (Scala)
        /opt/spark/bin/spark-shell --driver-class-path /usr/share/java/sqljdbc42.jar           
        </li>
        <li>
                Initialize the database
                :load initialize_db.scala
        </li>
        <li>
        Reset the tables
        :load initialize_tables.scala
        </li>
        <li>
        Initialize the tables
        :load load_data.scala
        </li>
        <li>
          Look at Spark Web UI and check the data with PowerBI
        </li>

        <li>
        Load Dates
        :load process_dim_date.scala
        </li>
        <li>
        Load Devices
        :load process_dim_device.scala
        </li>
        <li>
        Load Location
        :load process_dim_location.scala
        </li>
        <li>
        Load fact entry measure
        :load process_fact_measure.scala
        </li>
    
        <li>
        Load fact readings
        :load process_fact_measurement.scala
        </li>
    </ul> 
</li>
<li>   <h2>Python Instructions</h2>
    <ul>    
    <li>
    Start spark shell with the sql driver path (python)
    /opt/spark/bin/pyspark
    </li>
    <li>
    Initialize the database
    /opt/spark/bin/spark-submit /mnt/c/repos/dw-spark/spark/python/initialize_db.py
    </li>
    <li>
    Reset the tables
    TODO
    </li>
    <li>
    Initialize the tables
    TODO
    </li>

    <li>
    Load Dates
    TODO
    </li>
    <li>
    Load Devices
    TODO
    </li>
    <li>
    Load Location
    TODO
    </li>
    <li>
    Load fact entry measure
    TODO
    </li>

    <li>
    Load fact readings
    TODO
    </li>
</ul>
</li>
</ul>
Command Line: (from VSCode Terminal)

<p></p>
Bash
<p></p>
cd /spak/scala>
<p></p>
 /opt/spark/sbin/start-thriftserver.sh     
<p></p>
/opt/spark/bin/spark-shell --driver-class-path /usr/share/java/sqljdbc42.jar

<p></p>
Initialize and data load_data
<p></p>
:load initialize_db.scala
<p></p>
:load initialize_tables.scala
<p></p>
:load load_data.scala
<p></p>

Tranformation Process
<p></p>
:load process_dim_date.scala
<p></p>
:load process_dim_device.scala
<p></p>
:load process_dim_location.scala
<p></p>
:load process_fact_measure.scala
<p></p>
:load process_fact_measurement.scala

<p></p>
Visualize Data

<p></p>
Python Commands

./spark-submit /mnt/c/repos/dw-spark/spark/python/initialize_db.py
./spark-submit /mnt/c/repos/dw-spark/spark/python/initialize_tables.py